Args in experiment:
Namespace(activation='gelu', annealing_start=0.01, annealing_step=10, batch_size=32, c_out=7, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', dec_in=7, des='Exp', devices='4,5,6,7', distil=True, do_predict=False, dropout=0.05, e_layers=2, edl_loss='edl_mse', embed='timeF', embed_type=0, enc_in=7, etrans_func='softplus', factor=3, features='M', freq='h', gpu=0, hidden_dim=512, individual=False, is_training=1, itr=1, label_len=48, lambda_acl=1, lambda_cls=0.0001, lambda_direct=1, lambda_reg=1, learning_rate=1e-05, loss='mse', lradj='type1', model='Informer', model_id='ETTh1_96_720', moving_avg=25, n_heads=8, num_coarse=2, num_fine=4, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/ETT-small/', seq_len=96, target='OT', test_flop=False, train_epochs=30, train_only=False, use_amp=False, use_gpu=True, use_multi_gpu=False, use_span_weight=False, with_iw=True, with_kl=True, with_uc=True)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_96_720_Informer_ETTh1_ftM_sl96_ll48_pl720_lc0.0001_lr1e-05_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825 M
val 2161 M
test 2161 M
	iters: 100, epoch: 1 | loss: 15.5804367
	speed: 0.1018s/iter; left time: 734.8001s
	iters: 200, epoch: 1 | loss: 13.5234966
	speed: 0.0854s/iter; left time: 608.2771s
Epoch: 1 | 30 cost time: 22.6174s
Epoch: 1, Steps: 244 | Train Loss: 16.7319721 Vali Loss: 9.5663759 Test Loss: 8.9961208
Validation loss decreased (inf --> 9.566376).  Saving model ...
Updating learning rate to 1e-05
	iters: 100, epoch: 2 | loss: 12.2252140
	speed: 0.2056s/iter; left time: 1434.4828s
	iters: 200, epoch: 2 | loss: 11.6096535
	speed: 0.0839s/iter; left time: 577.3038s
Epoch: 2 | 30 cost time: 21.5187s
Epoch: 2, Steps: 244 | Train Loss: 11.5301250 Vali Loss: 8.5196528 Test Loss: 8.0172818
Validation loss decreased (9.566376 --> 8.519653).  Saving model ...
Updating learning rate to 5e-06
	iters: 100, epoch: 3 | loss: 10.9688282
	speed: 0.2034s/iter; left time: 1369.3292s
	iters: 200, epoch: 3 | loss: 10.8883247
	speed: 0.0860s/iter; left time: 570.2675s
Epoch: 3 | 30 cost time: 22.0373s
Epoch: 3, Steps: 244 | Train Loss: 10.4952486 Vali Loss: 8.1449698 Test Loss: 7.6617845
Validation loss decreased (8.519653 --> 8.144970).  Saving model ...
Updating learning rate to 2.5e-06
	iters: 100, epoch: 4 | loss: 10.4257317
	speed: 0.2041s/iter; left time: 1324.1807s
	iters: 200, epoch: 4 | loss: 10.4735165
	speed: 0.0835s/iter; left time: 533.2159s
Epoch: 4 | 30 cost time: 21.7379s
Epoch: 4, Steps: 244 | Train Loss: 10.0189738 Vali Loss: 8.0085530 Test Loss: 7.5162492
Validation loss decreased (8.144970 --> 8.008553).  Saving model ...
Updating learning rate to 1.25e-06
	iters: 100, epoch: 5 | loss: 10.2969627
	speed: 0.2096s/iter; left time: 1308.7737s
	iters: 200, epoch: 5 | loss: 10.3372831
	speed: 0.0841s/iter; left time: 516.6682s
Epoch: 5 | 30 cost time: 21.9531s
Epoch: 5, Steps: 244 | Train Loss: 9.8178168 Vali Loss: 7.9510878 Test Loss: 7.4609834
Validation loss decreased (8.008553 --> 7.951088).  Saving model ...
Updating learning rate to 6.25e-07
	iters: 100, epoch: 6 | loss: 10.1638250
	speed: 0.2067s/iter; left time: 1240.6906s
	iters: 200, epoch: 6 | loss: 10.2381992
	speed: 0.0870s/iter; left time: 513.4220s
Epoch: 6 | 30 cost time: 22.1668s
Epoch: 6, Steps: 244 | Train Loss: 9.7274928 Vali Loss: 7.9299052 Test Loss: 7.4357503
Validation loss decreased (7.951088 --> 7.929905).  Saving model ...
Updating learning rate to 3.125e-07
	iters: 100, epoch: 7 | loss: 10.1100769
	speed: 0.2092s/iter; left time: 1204.3026s
	iters: 200, epoch: 7 | loss: 10.1613493
	speed: 0.0856s/iter; left time: 484.4110s
Epoch: 7 | 30 cost time: 22.0768s
Epoch: 7, Steps: 244 | Train Loss: 9.6797654 Vali Loss: 7.9175999 Test Loss: 7.4186372
Validation loss decreased (7.929905 --> 7.917600).  Saving model ...
Updating learning rate to 1.5625e-07
	iters: 100, epoch: 8 | loss: 10.1164198
	speed: 0.2118s/iter; left time: 1167.5052s
	iters: 200, epoch: 8 | loss: 10.1500320
	speed: 0.0857s/iter; left time: 464.1202s
Epoch: 8 | 30 cost time: 22.6359s
Epoch: 8, Steps: 244 | Train Loss: 9.6544640 Vali Loss: 7.9098050 Test Loss: 7.4139175
Validation loss decreased (7.917600 --> 7.909805).  Saving model ...
Updating learning rate to 7.8125e-08
	iters: 100, epoch: 9 | loss: 10.2260170
	speed: 0.2169s/iter; left time: 1142.9004s
	iters: 200, epoch: 9 | loss: 10.0665064
	speed: 0.0874s/iter; left time: 452.0186s
Epoch: 9 | 30 cost time: 22.8237s
Epoch: 9, Steps: 244 | Train Loss: 9.6429085 Vali Loss: 7.9079047 Test Loss: 7.4109362
Validation loss decreased (7.909805 --> 7.907905).  Saving model ...
Updating learning rate to 3.90625e-08
	iters: 100, epoch: 10 | loss: 10.0278883
	speed: 0.2145s/iter; left time: 1077.6957s
	iters: 200, epoch: 10 | loss: 10.1450272
	speed: 0.0882s/iter; left time: 434.5512s
Epoch: 10 | 30 cost time: 22.7465s
Epoch: 10, Steps: 244 | Train Loss: 9.6347744 Vali Loss: 7.9022970 Test Loss: 7.4096966
Validation loss decreased (7.907905 --> 7.902297).  Saving model ...
Updating learning rate to 1.953125e-08
	iters: 100, epoch: 11 | loss: 10.1672869
	speed: 0.2140s/iter; left time: 1022.9311s
	iters: 200, epoch: 11 | loss: 10.2279081
	speed: 0.0880s/iter; left time: 412.0340s
Epoch: 11 | 30 cost time: 22.7419s
Epoch: 11, Steps: 244 | Train Loss: 9.6309459 Vali Loss: 7.9023674 Test Loss: 7.4066264
EarlyStopping counter: 1 out of 3
Updating learning rate to 9.765625e-09
	iters: 100, epoch: 12 | loss: 10.0070276
	speed: 0.2170s/iter; left time: 984.7413s
	iters: 200, epoch: 12 | loss: 10.0776520
	speed: 0.0893s/iter; left time: 396.2277s
Epoch: 12 | 30 cost time: 23.1935s
Epoch: 12, Steps: 244 | Train Loss: 9.6283605 Vali Loss: 7.9019214 Test Loss: 7.4082927
Validation loss decreased (7.902297 --> 7.901921).  Saving model ...
Updating learning rate to 4.8828125e-09
	iters: 100, epoch: 13 | loss: 10.2170734
	speed: 0.2171s/iter; left time: 931.8266s
	iters: 200, epoch: 13 | loss: 10.1228189
	speed: 0.0883s/iter; left time: 370.3495s
Epoch: 13 | 30 cost time: 22.8419s
Epoch: 13, Steps: 244 | Train Loss: 9.6278108 Vali Loss: 7.9030815 Test Loss: 7.4036695
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.44140625e-09
	iters: 100, epoch: 14 | loss: 10.0850172
	speed: 0.2183s/iter; left time: 883.8789s
	iters: 200, epoch: 14 | loss: 10.1148949
	speed: 0.0875s/iter; left time: 345.6802s
Epoch: 14 | 30 cost time: 23.1436s
Epoch: 14, Steps: 244 | Train Loss: 9.6270561 Vali Loss: 7.9028749 Test Loss: 7.4053984
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.220703125e-09
	iters: 100, epoch: 15 | loss: 10.0526142
	speed: 0.2111s/iter; left time: 803.1309s
	iters: 200, epoch: 15 | loss: 10.0950155
	speed: 0.0878s/iter; left time: 325.3447s
Epoch: 15 | 30 cost time: 22.6184s
Epoch: 15, Steps: 244 | Train Loss: 9.6274759 Vali Loss: 7.9013771 Test Loss: 7.4074098
Validation loss decreased (7.901921 --> 7.901377).  Saving model ...
Updating learning rate to 6.103515625e-10
	iters: 100, epoch: 16 | loss: 10.1300688
	speed: 0.2120s/iter; left time: 754.9509s
	iters: 200, epoch: 16 | loss: 10.1507177
	speed: 0.0850s/iter; left time: 294.2920s
Epoch: 16 | 30 cost time: 22.3815s
Epoch: 16, Steps: 244 | Train Loss: 9.6271916 Vali Loss: 7.9013091 Test Loss: 7.4061637
Validation loss decreased (7.901377 --> 7.901309).  Saving model ...
Updating learning rate to 3.0517578125e-10
	iters: 100, epoch: 17 | loss: 10.1384897
	speed: 0.2129s/iter; left time: 706.1260s
	iters: 200, epoch: 17 | loss: 10.0778332
	speed: 0.0882s/iter; left time: 283.6901s
Epoch: 17 | 30 cost time: 22.6502s
Epoch: 17, Steps: 244 | Train Loss: 9.6262366 Vali Loss: 7.9016789 Test Loss: 7.4076608
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.52587890625e-10
	iters: 100, epoch: 18 | loss: 10.1538000
	speed: 0.2102s/iter; left time: 645.9103s
	iters: 200, epoch: 18 | loss: 10.1138687
	speed: 0.0872s/iter; left time: 259.2936s
Epoch: 18 | 30 cost time: 22.6203s
Epoch: 18, Steps: 244 | Train Loss: 9.6273604 Vali Loss: 7.9023072 Test Loss: 7.4083720
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.62939453125e-11
	iters: 100, epoch: 19 | loss: 10.1228514
	speed: 0.2131s/iter; left time: 602.7887s
	iters: 200, epoch: 19 | loss: 10.0910025
	speed: 0.0877s/iter; left time: 239.2716s
Epoch: 19 | 30 cost time: 22.5265s
Epoch: 19, Steps: 244 | Train Loss: 9.6262146 Vali Loss: 7.9027668 Test Loss: 7.4054606
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_96_720_Informer_ETTh1_ftM_sl96_ll48_pl720_lc0.0001_lr1e-05_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161 M
rmse:1.0519312620162964, mse:1.1065593957901, mae:0.8171797394752502, corr:[ 4.12570126e-03 -6.14107400e-03  3.37842368e-02 -2.43120566e-02
  3.38547071e-03 -3.05085871e-02 -2.69789156e-03  3.46711390e-02
 -2.05376428e-02  2.88412091e-03  6.15149178e-03  3.36041581e-03
  1.92635302e-02 -6.56868611e-03 -1.70110036e-02  2.31812540e-02
  8.66878405e-03 -3.06393951e-02 -3.03515457e-02 -2.18382734e-03
 -1.90034360e-02 -2.37815287e-02  1.02048572e-02  3.41741517e-02
 -1.69437379e-02 -1.76327565e-04  2.41992977e-02 -3.13784741e-03
  4.34832415e-03  3.42845656e-02  6.77211629e-03 -1.79364700e-02
 -1.33550037e-02 -2.05992209e-03  3.93805560e-03 -6.35242974e-03
 -1.70537177e-02  2.97221225e-02 -7.24518206e-03 -5.68419369e-03
 -2.22018957e-02  1.24708225e-03  2.80682538e-02 -6.26343302e-04
 -3.29950973e-02  8.83294363e-03 -6.40177692e-04 -1.79008637e-02
 -3.33553702e-02  2.67962310e-02  8.36887676e-03 -3.63699673e-03
 -2.68387329e-02 -1.11696804e-02  3.32978629e-02 -2.48297621e-02
 -2.19221283e-02 -2.55196169e-02 -1.54099735e-02 -2.53743734e-02
 -3.14942338e-02  1.50217453e-03 -5.91042871e-03 -1.04853092e-02
 -9.56831034e-03 -3.30951326e-02 -1.20147308e-02 -1.53073967e-02
 -1.88900698e-02  3.53771895e-02  4.42753360e-03  8.78097489e-03
  1.13435620e-02 -7.30839139e-03 -6.81148376e-03  7.02719670e-03
  9.53647867e-03 -7.24440860e-03  4.52746404e-04  2.34845951e-02
  8.81940033e-03  6.66067936e-04 -2.47261990e-02 -3.40396278e-02
 -1.43956952e-02  6.41166372e-03 -2.26085801e-02 -2.53121164e-02
  5.47600538e-03  2.48529986e-02 -1.47842467e-02 -7.71014299e-03
  1.90805402e-02 -2.78151296e-02  5.44948562e-04 -2.01858953e-02
 -2.92106494e-02  1.36448294e-02  3.35014090e-02 -2.65330095e-02
 -4.29550298e-02 -1.98219810e-02 -6.45959936e-03  2.17768066e-02
  4.98735951e-03 -1.05364248e-02  6.56735525e-03  1.90554410e-02
 -1.69772021e-02 -3.90092656e-02 -3.01035307e-02  4.48646070e-03
  2.77811643e-02  1.17144035e-03 -1.00201264e-03 -4.17687092e-03
 -5.83301531e-03 -1.41279027e-02 -2.14525908e-02 -2.58953706e-03
 -1.76303256e-02  2.87547149e-02 -8.38682428e-03  1.26917837e-02
  8.02502874e-03  2.37520076e-02 -1.08207501e-02  9.77066718e-03
 -1.00351637e-02 -3.19289975e-02  8.06966855e-05 -2.38634218e-02
 -1.32364826e-02  3.21332961e-02  1.55936154e-02  1.86359286e-02
 -2.30789720e-03  7.26293446e-03 -1.47109767e-02  1.19925989e-02
 -1.07024228e-02  3.04960500e-04  4.00246754e-02 -2.87945662e-02
 -6.85098255e-03 -1.79603472e-02 -2.50774026e-02  2.61528417e-02
  5.28885890e-03 -2.10235752e-02 -2.32418432e-04 -9.90801677e-03
  1.34486901e-02 -3.12255993e-02  1.85576431e-03 -1.35298027e-02
 -1.76078379e-02 -3.55802267e-03  1.14234230e-02 -6.59806840e-03
 -1.42821092e-02  7.88960885e-03 -2.17854735e-02  1.44324172e-02
  1.83033198e-02 -7.27057038e-03  1.61786675e-02 -4.73645981e-03
 -2.90606711e-02 -1.84752457e-02 -1.10857138e-04 -1.82737615e-02
  1.89631917e-02  1.73305012e-02  8.51986837e-03  1.38424216e-02
 -3.25304316e-03  2.28719916e-02 -2.13758107e-02  1.64656248e-02
  2.90786978e-02 -1.77810304e-02 -2.16246620e-02  1.69136636e-02
  7.30964355e-03 -1.15718842e-02  3.35356500e-03 -6.98537147e-03
 -1.87084880e-02 -3.08525399e-03  3.91215310e-02 -2.19094511e-02
  1.75895337e-02  1.06840385e-02  1.99037208e-03 -1.02024563e-02
 -3.80442850e-02  3.24747409e-03 -2.28662025e-02 -4.98182094e-03
 -3.47803906e-02 -4.29731123e-02 -2.72883964e-03  2.14957092e-02
 -2.41329335e-02  1.62335951e-02  5.32809645e-04 -3.33860330e-02
 -5.38572110e-03 -1.31804161e-02 -9.11127403e-03  1.99447814e-02
 -1.26790442e-02 -4.57491865e-03  3.27689573e-02 -6.32920256e-03
 -4.41357866e-03 -9.25173052e-04  6.97820727e-03 -1.63263492e-02
 -7.08510820e-03  1.43164247e-02 -2.44170874e-02  1.14344591e-02
  2.44003534e-02  2.09736128e-04  9.83726326e-03 -7.82568473e-03
 -2.17543431e-02 -4.96360287e-03  3.68124843e-02 -2.34022662e-02
 -1.64457839e-02 -3.53762135e-02  1.60299931e-02 -1.61163434e-02
 -2.94754785e-02  8.03805050e-03 -4.39163530e-03 -1.14394939e-02
 -9.18326154e-03 -3.78576070e-02  4.86336835e-02 -1.01307482e-02
 -6.70770742e-03 -2.08873674e-03  1.42861437e-02 -9.89602646e-04
  1.40359951e-02  2.03597117e-02 -4.64560017e-02  2.66637672e-02
 -1.51240043e-02 -1.41830379e-02 -2.46313345e-02 -7.90234283e-03
 -1.69832204e-02 -3.35078426e-02 -1.07635139e-02  3.69540788e-02
 -3.64682600e-02  1.91333760e-02 -9.25685838e-03 -1.51215680e-02
 -1.34376884e-02  4.08252561e-03  3.23706563e-03 -2.42937710e-02
 -6.05238741e-03 -2.47168839e-02 -1.15611553e-02 -2.40955688e-02
  4.44572978e-03  5.53493388e-03  1.66290682e-02 -8.39503948e-03
 -2.32296321e-03  4.75388253e-03 -1.86487054e-03 -1.77268498e-02
  1.91761218e-02 -2.42882278e-02  1.94425024e-02 -8.50535836e-03
 -2.39105839e-02 -1.63016655e-02 -1.78732648e-02  2.85448618e-02
  1.08375307e-02  4.68767155e-03 -1.49711929e-02  2.79970956e-03
  1.81755808e-03 -2.66684741e-02 -2.76114810e-02  3.30700167e-02
  8.09280295e-03 -3.80928405e-02 -4.10389565e-02  3.28084524e-03
 -1.25121111e-02 -3.32112759e-02  1.70766506e-02 -2.41931006e-02
  1.02208192e-02 -1.48662347e-02  2.22019665e-02 -1.21444399e-02
 -1.27129210e-02 -1.93485152e-03  9.76064149e-03 -9.57778003e-03
 -1.25134690e-02 -5.95343765e-03 -3.37330289e-02 -4.42544483e-02
 -2.17474550e-02  6.92477264e-03 -2.47919350e-03 -6.86056679e-03
  1.46974875e-02 -7.52481585e-03 -1.29438909e-02  1.41073354e-02
 -1.41352043e-03  1.86837073e-02 -3.67446267e-03 -3.38912681e-02
  2.85792560e-03 -7.10906135e-03 -6.76959148e-03 -1.91156063e-02
 -2.19394900e-02 -3.33917397e-03 -2.38821027e-03 -2.98585463e-02
 -4.80610458e-03  6.39687246e-03  9.06103943e-03 -3.52585316e-02
  2.13797707e-02 -6.74606184e-04 -1.59401000e-02 -2.24432442e-02
 -2.05222461e-02  1.46849370e-02 -5.47294952e-02 -5.10964952e-02
 -2.74167079e-02  4.29586694e-02 -1.91016104e-02  7.51646585e-04
 -2.01841034e-02 -1.04682595e-02 -5.70845278e-03 -3.33796367e-02
  1.08941719e-02 -1.20034004e-02 -3.09087317e-02  8.41839146e-03
  4.92917001e-03 -9.06157400e-03 -1.33753521e-02  1.39498804e-02
 -1.33170402e-02 -1.50595307e-02 -3.30471657e-02 -1.59424152e-02
  2.23360863e-02  2.49305312e-02 -2.48304345e-02 -2.36137938e-02
 -1.68284308e-02 -2.89320126e-02  2.79102102e-02 -8.83305445e-03
 -1.27400011e-02 -3.05363233e-03 -2.10992973e-02 -2.31162496e-02
 -6.16888097e-03  2.72778943e-02  4.68165206e-04  6.15781220e-03
 -1.30995002e-03 -7.83233251e-03 -6.79023378e-03 -1.72116254e-02
  4.21653315e-02  2.23336909e-02 -1.61625817e-02 -9.89845674e-03
 -2.19677109e-02 -3.21317487e-03  2.24998835e-02 -1.04451682e-02
 -9.11754742e-03 -1.08671170e-02  1.14958584e-02  3.15958150e-02
 -3.28363676e-05 -1.58642023e-03 -9.92909539e-03 -1.05804289e-02
  9.55915265e-03  1.10042924e-02  1.55152353e-02  1.74045078e-02
  2.66308375e-02 -1.66732236e-04 -6.51017530e-03 -2.28501391e-02
 -1.11676473e-02  1.63011886e-02  2.09227577e-03  1.58931799e-02
 -2.18352545e-02 -3.02596707e-02 -9.47287958e-03 -3.73110250e-02
 -2.68519460e-03 -1.11737847e-02 -7.55522167e-03 -3.24285626e-02
  1.41517608e-03 -2.05150787e-02  1.49472971e-02 -8.14480335e-03
  2.90881228e-02 -3.49876122e-03 -6.17202139e-03 -1.72355641e-02
 -3.04564536e-02 -6.27405709e-03 -3.66577357e-02 -2.07844749e-02
  2.14636065e-02 -1.99744981e-02  3.57165262e-02 -1.26561103e-02
  6.16202643e-03 -3.20444070e-02  1.97641738e-02  1.38296634e-02
 -4.08297181e-02 -7.50877487e-04 -9.65533219e-03 -2.48489585e-02
  2.94891950e-02 -3.15625174e-03 -1.03518041e-02  4.28339886e-03
 -1.66296046e-02  9.19541437e-03 -1.22643756e-02 -1.62710380e-02
  8.61183181e-03 -4.61099148e-02  2.38147341e-02 -2.92138022e-04
  1.86339102e-03  1.41157908e-02  1.22782113e-02  4.22428409e-03
  2.59063095e-02  3.31973769e-02 -3.14247869e-02 -1.99548397e-02
 -1.85328268e-03 -1.23664993e-03  7.99114630e-03 -2.57816426e-02
 -1.65207610e-02 -3.09243426e-02  1.69322994e-02 -3.09786852e-02
  6.46816380e-03 -8.63526668e-03 -2.48942394e-02  2.63810344e-02
 -4.66074655e-03 -4.24270378e-03  4.34172750e-02  8.69479124e-03
  3.57940532e-02  2.40206588e-02 -1.55026922e-02 -9.07846447e-03
 -4.19260934e-02  1.50618255e-02 -9.29131825e-03 -2.50138771e-02
  6.93072937e-03 -1.92835070e-02  1.41638070e-02 -9.98803321e-03
 -5.95851627e-04 -3.50884348e-02  6.64955657e-03  1.45845544e-02
  5.60593465e-03  1.55342547e-02 -1.16386125e-02  1.41914911e-03
  8.56423285e-03 -1.99147631e-04  2.60392930e-02  2.91886833e-02
  2.01767590e-02 -1.34346830e-02 -1.54094612e-02 -2.03414802e-02
  1.79144014e-02 -4.13968024e-04 -1.12067228e-02  2.05543879e-02
  3.69985588e-02 -7.83409644e-03  4.10064356e-03  1.29907541e-02
 -2.11663023e-02 -1.26442248e-02  5.54027176e-03 -5.86199574e-03
 -2.29778662e-02 -7.75015447e-03 -1.17515372e-02  2.41424306e-03
 -9.51324869e-03 -5.43085746e-02  1.19688008e-02 -2.37826407e-02
 -4.85338038e-03  8.74841399e-03 -2.44216956e-02  3.83926034e-02
 -1.05250329e-02 -4.10359465e-02  2.30581425e-02 -7.31134042e-03
 -1.98550131e-02 -1.58913545e-02 -4.18872535e-02 -9.82314069e-03
 -4.44148630e-02 -1.00253103e-02 -9.31075029e-03 -2.89975200e-02
 -3.99357826e-02  1.66590940e-02  1.45581644e-02  3.71125825e-02
 -1.44622205e-02 -5.86905750e-03  9.84851830e-03 -3.84867117e-02
 -7.79047282e-03 -3.17494050e-02 -9.02353600e-03 -3.17616202e-02
  3.30237672e-04  1.81984715e-02 -5.58450669e-02 -3.19540910e-02
  2.02953555e-02 -3.40662561e-02 -1.78776514e-02 -2.73352768e-02
 -3.36467177e-02 -1.06283575e-02 -3.92079772e-03 -9.97298118e-03
  2.99719777e-02 -1.39415334e-03 -2.30542105e-02  1.20096738e-02
  1.31748589e-02  6.80012582e-03 -2.82137562e-02  3.44837122e-02
 -6.89853495e-03 -4.84698229e-02 -4.08434346e-02 -1.41800940e-02
 -2.57797632e-02  1.01933656e-02 -3.15685407e-03 -3.73786986e-02
  1.26490463e-02 -2.87873708e-02 -1.90437641e-02  9.39751044e-04
  1.65350623e-02  7.22631440e-03  1.19326459e-02  1.55424271e-02
 -2.22247839e-02 -3.96442562e-02 -4.04734872e-02  1.80642307e-02
 -2.14366019e-02 -1.59211259e-03  3.35987657e-02  2.08245479e-02
 -3.03187240e-02  4.30040003e-04  6.68273494e-03  1.60647593e-02
 -4.53777537e-02 -1.66635420e-02 -2.54132841e-02 -4.54469770e-02
  2.31249467e-03 -1.30309798e-02 -1.38251893e-02  7.70890294e-03
 -1.80414580e-02 -4.64553311e-02 -5.40505676e-03 -3.42446356e-03
  2.78437920e-02 -2.64939535e-02 -5.27746826e-02 -2.12163255e-02
  3.74690220e-02 -1.00462474e-02 -1.51764406e-02  3.96008119e-02
 -2.66134180e-02  1.58147812e-02 -3.00695468e-02 -4.66818362e-02
  9.80515499e-03  9.00027715e-03 -2.43828129e-02 -2.66931392e-02
 -6.88447477e-03 -8.72474746e-04 -1.65760815e-02 -5.69541380e-03
 -2.05954723e-02  1.79518349e-02 -6.58714678e-03 -2.17253566e-02
  9.09444207e-05 -4.27630320e-02 -1.44408224e-02  6.31429954e-03
 -1.15368282e-02 -4.74026948e-02  9.33035184e-03  1.09798973e-02
  2.88682990e-04  3.78790163e-02 -1.06064444e-02  5.28662524e-04
 -4.53675017e-02 -9.78801213e-03 -5.01141436e-02 -3.69000062e-02
  3.52930613e-02 -9.93650686e-03 -2.07442679e-02 -8.56573414e-03
 -7.72095378e-03 -1.35788154e-02  1.22010792e-02  1.67306196e-02
 -2.64022872e-02  3.58522274e-02  4.34699189e-03 -3.91014330e-02
 -3.41114253e-02  1.65536832e-02 -4.57918979e-02  3.76915000e-02
  2.06835475e-03 -8.58849846e-03 -2.50955354e-02 -3.23911458e-02
  9.67729744e-03 -2.30305316e-03 -1.62082780e-02 -1.33707915e-02
 -1.10491710e-02 -2.08487865e-02 -4.93526869e-02 -2.43018288e-02
 -4.62134965e-02  1.79533206e-03 -9.11128521e-03 -6.80314470e-03
 -1.23352548e-02  6.67162891e-03 -1.75578110e-02 -1.06688857e-03
 -2.23947335e-02  2.11501643e-02  1.12252878e-02  4.53336956e-03
  2.18152069e-02  4.62851860e-02 -1.38139473e-02  9.62530205e-04
 -2.89538456e-03  2.59415153e-03  1.85032878e-02 -7.51598412e-03
 -3.32229994e-02 -2.86268606e-03 -2.89584161e-03 -3.23904157e-02
  6.48246752e-03  6.50184928e-03  3.42579931e-03 -1.34053761e-02
 -1.20972760e-03 -7.31465360e-03 -1.40456832e-03  3.25049926e-03]
