nohup: ignoring input
Args in experiment:
Namespace(activation='gelu', annealing_start=0.01, annealing_step=10, batch_size=32, c_out=7, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', dec_in=7, des='test', devices='4,5,6,7', distil=True, do_predict=False, dropout=0.05, e_layers=2, edl_loss='edl_mse', embed='timeF', embed_type=0, enc_in=7, etrans_func='softplus', factor=3, features='M', freq='h', gpu=0, hidden_dim=512, individual=False, is_training=1, itr=1, label_len=48, lambda_acl=1.0, lambda_cls=0.01, lambda_direct=1.0, lambda_reg=1.0, learning_rate=0.001, loss='mse', lradj='type1', model='Informer', model_id='ETTh1_96_96_HCAN', moving_avg=25, n_heads=8, num_coarse=2, num_fine=4, num_workers=10, output_attention=False, patience=3, pred_len=336, root_path='./dataset/ETT-small/', seq_len=96, target='OT', test_flop=False, train_epochs=30, train_only=False, use_amp=False, use_gpu=True, use_multi_gpu=False, use_span_weight=False, with_iw=True, with_kl=True, with_uc=True)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_96_96_HCAN_Informer_ETTh1_ftM_sl96_ll48_pl336_lc0.01_lr0.001_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_test_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8209 M
val 2545 M
test 2545 M
	iters: 100, epoch: 1 | loss: 322.3098450
	speed: 0.0834s/iter; left time: 631.9137s
	iters: 200, epoch: 1 | loss: 321.8922424
	speed: 0.0695s/iter; left time: 520.1350s
Epoch: 1 | 30 cost time: 19.5070s
Epoch: 1, Steps: 256 | Train Loss: 304.2808676 Vali Loss: 259.6417745 Test Loss: 260.4431262
Validation loss decreased (inf --> 259.641775).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 326.9816589
	speed: 0.1902s/iter; left time: 1393.0411s
	iters: 200, epoch: 2 | loss: 322.6199646
	speed: 0.0699s/iter; left time: 504.8152s
Epoch: 2 | 30 cost time: 19.6445s
Epoch: 2, Steps: 256 | Train Loss: 304.3217486 Vali Loss: 271.3903975 Test Loss: 271.8792250
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 320.8309326
	speed: 0.1850s/iter; left time: 1307.9549s
	iters: 200, epoch: 3 | loss: 319.8398438
	speed: 0.0683s/iter; left time: 475.9619s
Epoch: 3 | 30 cost time: 18.8535s
Epoch: 3, Steps: 256 | Train Loss: 301.3585067 Vali Loss: 257.8690970 Test Loss: 258.2798705
Validation loss decreased (259.641775 --> 257.869097).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 319.0995789
	speed: 0.1817s/iter; left time: 1237.8643s
	iters: 200, epoch: 4 | loss: 319.1099548
	speed: 0.0720s/iter; left time: 483.0394s
Epoch: 4 | 30 cost time: 19.0998s
Epoch: 4, Steps: 256 | Train Loss: 299.8542940 Vali Loss: 257.5788970 Test Loss: 257.9690656
Validation loss decreased (257.869097 --> 257.578897).  Saving model ...
Updating learning rate to 0.000125
	iters: 100, epoch: 5 | loss: 318.9249268
	speed: 0.1802s/iter; left time: 1181.2964s
	iters: 200, epoch: 5 | loss: 318.9260254
	speed: 0.0675s/iter; left time: 436.1682s
Epoch: 5 | 30 cost time: 18.7341s
Epoch: 5, Steps: 256 | Train Loss: 299.7041242 Vali Loss: 257.4813480 Test Loss: 257.9389186
Validation loss decreased (257.578897 --> 257.481348).  Saving model ...
Updating learning rate to 6.25e-05
	iters: 100, epoch: 6 | loss: 318.9864502
	speed: 0.1863s/iter; left time: 1173.5972s
	iters: 200, epoch: 6 | loss: 318.8352356
	speed: 0.0714s/iter; left time: 442.5466s
Epoch: 6 | 30 cost time: 19.5315s
Epoch: 6, Steps: 256 | Train Loss: 299.6520943 Vali Loss: 257.4102511 Test Loss: 257.8362725
Validation loss decreased (257.481348 --> 257.410251).  Saving model ...
Updating learning rate to 3.125e-05
	iters: 100, epoch: 7 | loss: 318.8989258
	speed: 0.1826s/iter; left time: 1103.9562s
	iters: 200, epoch: 7 | loss: 318.8905640
	speed: 0.0685s/iter; left time: 406.9792s
Epoch: 7 | 30 cost time: 18.9252s
Epoch: 7, Steps: 256 | Train Loss: 299.6266226 Vali Loss: 257.4049519 Test Loss: 257.8962846
Validation loss decreased (257.410251 --> 257.404952).  Saving model ...
Updating learning rate to 1.5625e-05
	iters: 100, epoch: 8 | loss: 318.8727722
	speed: 0.1854s/iter; left time: 1073.1403s
	iters: 200, epoch: 8 | loss: 318.9307861
	speed: 0.0689s/iter; left time: 391.7802s
Epoch: 8 | 30 cost time: 19.0881s
Epoch: 8, Steps: 256 | Train Loss: 299.6148866 Vali Loss: 257.4179407 Test Loss: 257.8790948
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-06
	iters: 100, epoch: 9 | loss: 318.9197083
	speed: 0.1825s/iter; left time: 1009.8104s
	iters: 200, epoch: 9 | loss: 318.8885803
	speed: 0.0693s/iter; left time: 376.7466s
Epoch: 9 | 30 cost time: 19.0324s
Epoch: 9, Steps: 256 | Train Loss: 299.6091819 Vali Loss: 257.4054740 Test Loss: 257.8487401
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-06
	iters: 100, epoch: 10 | loss: 318.8070374
	speed: 0.1868s/iter; left time: 985.5926s
	iters: 200, epoch: 10 | loss: 318.8867493
	speed: 0.0720s/iter; left time: 372.4895s
Epoch: 10 | 30 cost time: 19.5600s
Epoch: 10, Steps: 256 | Train Loss: 299.6054745 Vali Loss: 257.4061570 Test Loss: 257.8449286
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh1_96_96_HCAN_Informer_ETTh1_ftM_sl96_ll48_pl336_lc0.01_lr0.001_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_test_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545 M
rmse:1.1368417739868164, mse:1.2924093008041382, mae:0.9324871897697449, corr:[0.08910893 0.08456416 0.08318072 0.08461957 0.0848638  0.08809614
 0.08350799 0.07860186 0.07430863 0.06781667 0.07600784 0.07071336
 0.07283504 0.07159745 0.06095366 0.06862637 0.06957626 0.06853704
 0.06638556 0.06251983 0.05963257 0.06630863 0.06612869 0.06459315
 0.07037849 0.07758289 0.07854281 0.07436513 0.0775275  0.07623677
 0.07645418 0.06615641 0.06852249 0.06776944 0.06797062 0.0657978
 0.06986941 0.06613777 0.06611387 0.0682074  0.06487241 0.06255933
 0.0609267  0.06478393 0.05779381 0.05870709 0.05926322 0.06650449
 0.06783503 0.07260653 0.06720805 0.07225393 0.07365225 0.07194571
 0.06925504 0.06539926 0.06263407 0.06109571 0.06058427 0.06187743
 0.05804601 0.059548   0.06073382 0.06139362 0.0583078  0.05755477
 0.05533868 0.05379746 0.05276527 0.05534511 0.05287692 0.05365575
 0.05961785 0.06283805 0.06939365 0.0682396  0.06927823 0.06787473
 0.06264676 0.0609006  0.06005253 0.05806235 0.0573433  0.05871114
 0.05549866 0.05954305 0.05920507 0.05421006 0.05642826 0.05302304
 0.05593261 0.0554865  0.05366983 0.05624425 0.05533471 0.06235086
 0.05965563 0.07040823 0.06837136 0.06373015 0.06935839 0.06856468
 0.06532272 0.06331472 0.05972086 0.05936135 0.05812074 0.05684167
 0.05845867 0.05360019 0.05588864 0.05603901 0.0561438  0.05372151
 0.05305426 0.05386274 0.04850587 0.05109983 0.05077544 0.0573209
 0.06250208 0.05917903 0.06548078 0.06599152 0.06387016 0.06498478
 0.06065042 0.05904559 0.05981758 0.05207859 0.05633504 0.05275039
 0.05630967 0.05196296 0.05294245 0.05123631 0.05352238 0.05135472
 0.05133602 0.04809305 0.05030239 0.0469684  0.04979119 0.05105328
 0.05066771 0.05785405 0.06092097 0.06411118 0.0593365  0.06097146
 0.05986901 0.05945496 0.05493989 0.05759869 0.05448846 0.05814789
 0.05521338 0.05130218 0.05213462 0.05025205 0.05067533 0.0536283
 0.05271069 0.04643092 0.04940014 0.04481139 0.04859531 0.0555079
 0.05283652 0.05990582 0.06035243 0.05774176 0.06048004 0.06244969
 0.05654737 0.05703942 0.05282822 0.05103701 0.05251263 0.04793006
 0.04953814 0.04920473 0.04871362 0.04989533 0.04605282 0.04839196
 0.04424293 0.04457846 0.04417872 0.04413971 0.04515731 0.04997574
 0.0525784  0.05854258 0.05931873 0.06013003 0.06079474 0.05922742
 0.05905009 0.05500581 0.05315342 0.05194187 0.04890829 0.04479327
 0.04681739 0.04428446 0.04587438 0.04187432 0.04282769 0.04272014
 0.04330387 0.04168431 0.04216155 0.04324654 0.04252538 0.04754505
 0.05096575 0.05752375 0.05755407 0.0606807  0.05564896 0.05550712
 0.05186344 0.05194391 0.04837311 0.04587601 0.04229134 0.04301699
 0.04264205 0.04215688 0.03836579 0.03978647 0.03734805 0.03829855
 0.03813571 0.03516144 0.03919666 0.03965731 0.04038231 0.04774822
 0.04992533 0.0533041  0.05234533 0.05652261 0.05651428 0.05244498
 0.04869315 0.04802427 0.04268504 0.04577191 0.04180423 0.03897093
 0.03989764 0.03824307 0.03995625 0.03429385 0.03298346 0.03974985
 0.03296987 0.03695586 0.0341334  0.0347646  0.03770646 0.03970556
 0.04593782 0.05043129 0.05113028 0.05447869 0.04982372 0.05054185
 0.04971854 0.04787276 0.03903827 0.03988022 0.04146157 0.04260767
 0.03951797 0.03770981 0.03770568 0.03766745 0.0361215  0.03568323
 0.0381071  0.03912522 0.03605064 0.03549463 0.03840056 0.04526144
 0.04750679 0.05538892 0.05584599 0.06075263 0.06241956 0.056674
 0.05333145 0.05238827 0.04724955 0.04638137 0.04624934 0.04736848
 0.04481936 0.03977025 0.04268118 0.04147529 0.0390076  0.04186154
 0.03989034 0.0402247  0.04039955 0.042923   0.04194145 0.04226953
 0.04980405 0.05151673 0.05572994 0.05654226 0.06125059 0.05794672
 0.05578272 0.04863486 0.05211398 0.04700918 0.04674852 0.04439974
 0.04711328 0.045783   0.04512601 0.04174519 0.04027715 0.04375017
 0.04133114 0.03898295 0.0409731  0.04301738 0.04501935 0.04663638]
